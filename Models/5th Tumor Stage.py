# -*- coding: utf-8 -*-
"""Stages.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_85utB--aXGspX62Sp66k4-3w0un_Hr6
"""

# -*- coding: utf-8 -*-
import seaborn as sns; sns.set(color_codes=True)  # visualization tool
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import os

import statistics
import collections
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from datetime import datetime
from datetime import date
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import (
                    MultinomialNB, 
                    GaussianNB, 
                    BernoulliNB
                    )
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.svm import (
                    SVC, 
                    NuSVC, 
                    LinearSVC
                    )
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing  import StandardScaler
from sklearn.decomposition import PCA
from sklearn import tree
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,confusion_matrix

from IPython.display import display 
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC

import keras
from keras.models import Sequential , Model
from keras.layers import (
                        Dense,
                        Conv2D,
                        MaxPool2D,
                        Flatten,
                        Dropout,
                        MaxPooling2D,
                        Input,
                        Conv2DTranspose,
                        Concatenate,
                        BatchNormalization,
                        UpSampling2D,
                        AveragePooling2D,
                        GlobalAveragePooling2D,
                        Activation,
                        ZeroPadding2D
                        )
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam , SGD
from keras.layers.merge import concatenate
from keras.layers.advanced_activations import LeakyReLU
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from keras import backend as K
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding

#from keras.utils import plot_model

from sklearn.metrics import classification_report,confusion_matrix
from PIL import Image
import tensorflow as tf
import glob
import random
import cv2
from random import shuffle
import itertools
import imutils

!unzip sample_data/images_data.zip -d sample_data/

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

patient_info = drive.CreateFile({'id':"1dN1peOSeugjLLzzazh4yGoy_g1LBlzp7"})
patient_info.GetContentFile("patient_info.csv")
patient_info = pd.read_csv("patient_info.csv")

patient_info["Tumor_class_label"] = patient_info["Stage"]
cleanup_nums = {
    
    "Tumor_class_label":{"I":0 ,"II":1,"III":2,"IIII":3}
}
patient_info = patient_info.replace(cleanup_nums)

del cleanup_nums

patient_info.info()

patient_info.sample(5)

patient_info = patient_info[patient_info['Tumor_Type'] != "Null"]

patient_info["Stage"].value_counts()

labels = {0:"I",1:"II",2:"III",3:"IIII"}

def get_data (data_dir , target ):
    X = list()
    y=list()
    img_size = 256
    for index, row in patient_info.iterrows():
        path = os.path.join(data_dir, str (row['Patient_Num']))
        label = row[target]
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img))[...,::-1]
                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size   
                X.append(resized_arr)
                y.append(label)
            except Exception as e:
                print(e , row['Patient_Num'] )
    return X , y

X , y  = get_data("sample_data/Dalia_Data/", target = "Tumor_class_label")

dict(zip(list(y),[list(y).count(i) for i in list(y)]))

plt.figure(figsize = (5,5))
plt.imshow(X[20])
plt.title(labels[y[20]])
plt.show()

plt.figure(figsize = (5,5))
plt.imshow(X[3220])
plt.title(labels[y[3220]])
plt.show()

plt.figure(figsize = (5,5))
plt.imshow(X[3080])
plt.title(labels[y[3080]])
plt.show()

plt.figure(figsize = (5,5))
plt.imshow(X[3990])
plt.title(labels[y[3990]])
plt.show()

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.20)

print ("Number images for training : {}".format(len (x_train)))
print ("Number images for testing : {}".format(len (x_test)))
print ("Number images for Validation : {}".format(len (x_val)))

def data_prepare (X , y , folder_name , labels ) :
    path = "sample_data/{}".format(folder_name)
    os.mkdir(path)
    # create folder for labels 
    for key , value in labels.items()  : 
        path = "sample_data/{}/{}".format(folder_name,value)
        os.mkdir(path)

    if len (X) != len (y) : 
      print ("error size data X and y is not equal")
      return 

    for index , value in enumerate(y) : 
      im = Image.fromarray(X[index])
      path = "sample_data/{}/{}/{}.jpeg".format(folder_name,labels[value],str(index))
      im.save(path)
    return

data_prepare (X=x_train ,y=y_train ,folder_name="train", labels=labels )
data_prepare (X=x_test ,y=y_test ,folder_name="test", labels=labels )
data_prepare (X=x_val ,y=y_val ,folder_name="validation", labels=labels )

## Genration Images 

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)


test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('/content/sample_data/train',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')

test_set = test_datagen.flow_from_directory('/content/sample_data/test',
                                            target_size = (224,224),
                                            batch_size = 32,
                                            class_mode = 'categorical')

# model= Sequential()
# model.add(Conv2D(32, (3, 3), input_shape = (224, 224, 3), activation = 'relu'))
# model.add(MaxPooling2D(pool_size = (2, 2)))
# model.add(Flatten())
# model.add(Dense(units = 128, activation = 'relu'))
# model.add(Dense(units = 4, activation = 'softmax'))
# model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=["accuracy"])


class AccuracyStopping(keras.callbacks.Callback):
    def __init__(self, acc_threshold):
        super(AccuracyStopping, self).__init__()
        self._acc_threshold = acc_threshold

    def on_epoch_end(self, batch, logs={}):
        train_acc = logs.get('accuracy')
        print(train_acc)
        value=1-train_acc
        print(value)
        self.model.stop_training = value <= self._acc_threshold

acc_callback = AccuracyStopping(0.02)

def get_Model():
    modelName= Sequential()
    modelName.add(BatchNormalization(input_shape = (224,224,3)))
    modelName.add(Conv2D(32, (3, 3), input_shape = (224, 224, 3), activation = 'relu'))
    modelName.add(MaxPooling2D(pool_size = (2, 2)))
    modelName.add(Dropout(0.25))
    modelName.add(Flatten())
    modelName.add(Dense(units = 128, activation = 'relu'))
    modelName.add(Dense(units = 4, activation = 'softmax'))
    return modelName

x=get_Model()

x.compile(
    optimizer='adam' , 
    loss ="categorical_crossentropy", 
    metrics=['accuracy']    )

history = x.fit(    training_set,
                    steps_per_epoch = (2688 /32),
                    epochs=10, 
                    validation_data=test_set,
                    validation_steps = (840 /32))

model =  x
model.summary()

hist=model.fit(training_set,
                         steps_per_epoch = (3360 /128),
                         epochs = 30,
                         validation_data = test_set,
                         validation_steps = (672 /128))

loss,accuracy=model.evaluate(test_set)
print (f"Test Loss     = {loss}")
print (f"Test Accuracy = {accuracy}")

print('Training Set Clases')
print(training_set.class_indices)
print("=="*10)
print('Testing Set Clases')
print(test_set.class_indices)

from keras.preprocessing import image

path='/content/sample_data/validation/I'
l_I =[]

filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]
y_I =[0]*len(filelist)
print ("Number of images for I :" , len (filelist))

for img in filelist:
  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))
  test_image = image.img_to_array(test_image)
  test_image = np.expand_dims(test_image, axis = 0)
  l_I.append(test_image)

l_I_result=[]
for i in range(len(l_I)):
  xx = model.predict_classes(l_I[i])
  l_I_result.append(xx)

l_I_draw=[]
for i in range(len(l_I_result)):
    if (l_I_result[i][0] == 0):
        l_I_draw.append("I")
    elif (l_I_result[i][0] == 1):
        l_I_draw.append("II")
    elif (l_I_result[i][0] == 2):
        l_I_draw.append("III")
    else :
        l_I_draw.append("IIII")

display('==='*10)
display(dict(zip(list(l_I_draw),[list(l_I_draw).count(i) for i in list(l_I_draw)])))
display('==='*10)

sns.set_style('darkgrid')
sns.countplot(l_I_draw)
plt.show()

from keras.preprocessing import image

path='/content/sample_data/validation/II'
l_II =[]

filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]
y_II =[1]*len(filelist)
print ("Number of images for II :" , len (filelist))

for img in filelist:
  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))
  test_image = image.img_to_array(test_image)
  test_image = np.expand_dims(test_image, axis = 0)
  l_II.append(test_image)

l_II_result=[]
for i in range(len(l_II)):
  xx = model.predict_classes(l_II[i])
  l_II_result.append(xx)

l_II_draw=[]
for i in range(len(l_II_result)):
    if (l_II_result[i][0] == 0):
        l_II_draw.append("I")
    elif (l_II_result[i][0] == 1):
        l_II_draw.append("II")
    elif (l_II_result[i][0] == 2):
        l_II_draw.append("III")
    else :
        l_II_draw.append("IIII")

display('==='*10)
display(dict(zip(list(l_II_draw),[list(l_II_draw).count(i) for i in list(l_II_draw)])))
display('==='*10)

sns.set_style('darkgrid')
sns.countplot(l_II_draw)
plt.show()

from keras.preprocessing import image

path='/content/sample_data/validation/III'
l_III =[]

filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]
y_III =[2]*len(filelist)
print ("Number of images for III :" , len (filelist))

for img in filelist:
  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))
  test_image = image.img_to_array(test_image)
  test_image = np.expand_dims(test_image, axis = 0)
  l_III.append(test_image)

l_III_result=[]
for i in range(len(l_III)):
  xx = model.predict_classes(l_III[i])
  l_III_result.append(xx)

l_III_draw=[]
for i in range(len(l_III_result)):
    if (l_III_result[i][0] == 0):
        l_III_draw.append("I")
    elif (l_III_result[i][0] == 1):
        l_III_draw.append("II")
    elif (l_III_result[i][0] == 2):
        l_III_draw.append("III")
    else :
        l_III_draw.append("IIII")

display('==='*10)
display(dict(zip(list(l_III_draw),[list(l_III_draw).count(i) for i in list(l_III_draw)])))
display('==='*10)

sns.set_style('darkgrid')
sns.countplot(l_III_draw)
plt.show()

from keras.preprocessing import image

path='/content/sample_data/validation/IIII'
l_IIII =[]

filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]
y_IIII =[3]*len(filelist)
print ("Number of images for III :" , len (filelist))

for img in filelist:
  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))
  test_image = image.img_to_array(test_image)
  test_image = np.expand_dims(test_image, axis = 0)
  l_IIII.append(test_image)

l_IIII_result=[]
for i in range(len(l_IIII)):
  xx = model.predict_classes(l_IIII[i])
  l_IIII_result.append(xx)

l_IIII_draw=[]
for i in range(len(l_IIII_result)):
    if (l_IIII_result[i][0] == 0):
        l_IIII_draw.append("I")
    elif (l_IIII_result[i][0] == 1):
        l_IIII_draw.append("II")
    elif (l_IIII_result[i][0] == 2):
        l_IIII_draw.append("III")
    else :
        l_IIII_draw.append("IIII")

display('==='*10)
display(dict(zip(list(l_IIII_draw),[list(l_IIII_draw).count(i) for i in list(l_IIII_draw)])))
display('==='*10)

sns.set_style('darkgrid')
sns.countplot(l_IIII_draw)
plt.show()

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc

print('Training Set Clases')
print(training_set.class_indices)
print('Testing Set Clases')
print(test_set.class_indices)
print("======"*10)
print('\nConfusion Matrix')
print('Classification Report')
target_names = ['I', 'II','III','IIII']

y_labels  = y_I +y_II + y_III + y_IIII 
x_results = l_I_result + l_II_result + l_III_result + l_IIII_result
print(classification_report( y_labels , x_results , target_names=target_names))

hist = history
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']
loss = hist.history['loss']
val_loss = hist.history['val_loss']

plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.plot( acc, label='Training Accuracy')
plt.plot( val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 2, 2)
plt.plot( loss, label='Training Loss')
plt.plot( val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model.save("Tumer_Malignant.h5")
np.save('Tumer_Malignant_history_traning.npy',hist.history)